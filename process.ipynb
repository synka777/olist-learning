{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f81c0f",
   "metadata": {},
   "source": [
    "## Processing ETL des sets de donn√©es Olist\n",
    "\n",
    "#### 1. Nettoyage: Op√©rations globales √† tous les fichiers source\n",
    "\n",
    "Dans cette partie je g√®re les nettoyages qui peuvent potentiellement s'appliquer √† toutes les tables, sans faire de traitement sp√©cifique pour telle ou telle table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08bc36cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading olist_orders_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathieu/prj/olist-learning/scripts/utils.py:13: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col],\n",
      "/home/mathieu/prj/olist-learning/scripts/utils.py:13: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col],\n",
      "/home/mathieu/prj/olist-learning/scripts/utils.py:13: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col],\n",
      "/home/mathieu/prj/olist-learning/scripts/utils.py:13: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading olist_products_dataset.csv...\n",
      "Loading olist_order_items_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathieu/prj/olist-learning/scripts/utils.py:13: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading product_category_name_translation.csv...\n",
      "Loading olist_sellers_dataset.csv...\n",
      "Loading olist_geolocation_dataset.csv...\n",
      "Loading olist_order_reviews_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathieu/prj/olist-learning/scripts/utils.py:13: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col],\n",
      "/home/mathieu/prj/olist-learning/scripts/utils.py:13: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading olist_order_payments_dataset.csv...\n",
      "Loading olist_customers_dataset.csv...\n"
     ]
    }
   ],
   "source": [
    "# Import de d√©pendances et chargement des datasets en m√©moire vive\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from scripts.utils import clean_data\n",
    "\n",
    "data = {}\n",
    "\n",
    "rt = Path('./data')\n",
    "file_paths = os.listdir(rt)\n",
    "csv_file_paths = [f for f in file_paths if f.endswith('.csv')]\n",
    "for csv_path in csv_file_paths:\n",
    "    print(f'Loading {csv_path}...')\n",
    "    df = pd.read_csv(rt.joinpath(csv_path))\n",
    "    df = clean_data(df) # Utilisation d'un heler g√©n√©rique de nettoyage\n",
    "    source = csv_path.replace('olist_', '').replace('.csv', '').replace('_dataset', '')\n",
    "    data[source] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70be187",
   "metadata": {},
   "source": [
    "#### 2. Nettoyage: Cas sp√©cifiques\n",
    "\n",
    "Dans cette partie du notebook j'applique des netoyages plus sp√©cifiques, sur des tables en particulier:\n",
    "- customers: suppression de doublons et d\"une colonne non utilis√©e => EDIT: Pas une bonne id√©e, je commente\n",
    "- geolocation et customers: normalisation des noms de ville pour coh√©rence des √©ventuels PKI qui utiliseraient les noms de ville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des doublons de donn√©es table customers\n",
    "# data['customers'].drop_duplicates(subset=['customer_unique_id'], keep='first')\n",
    "# data['customers'] = data['customers'].drop(columns=['customer_unique_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e48d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import norm\n",
    "\n",
    "# Normalise les noms de villes\n",
    "data['geolocation']['geolocation_city'] = data['geolocation']['geolocation_city'].apply(norm)\n",
    "data['customers']['customer_city'] = data['customers']['customer_city'].apply(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8045d4",
   "metadata": {},
   "source": [
    "#### 3. Transformations\n",
    "\n",
    "La transformation principale que j'ai imagin√©e sur ce cas d'√©cole a √©t√© d'ajouter les traductions de noms de cat√©gories directement dans la table produits.\n",
    "\n",
    "Le but √©tant de simplifier les requ√™tes devant filtrer ou grouper par cat√©gorie, cela √©vite d'avoir √† faire des jointures ult√©rieures pour r√©cup√©rer les traductions.\n",
    "\n",
    "Au d√©part j'ai m√™me consid√©r√© faire cela pour cmpl√®tement supprimer la table de traductions, mais j'ai d√©cid√© de la garder, dans le sc√©nario o√π un utilisateur br√©silien aurait besoin de faire des requ√™tes sur la base de donn√©es: dans ce cas les jointures resteront faisables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ce17c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_category_name product_category_name_en\n",
      "0             perfumaria                perfumery\n",
      "1                  artes                      art\n",
      "2          esporte_lazer           sports_leisure\n",
      "3                  bebes                     baby\n",
      "4  utilidades_domesticas               housewares\n"
     ]
    }
   ],
   "source": [
    "# Transformation de la table des produits\n",
    "\n",
    "# Merge des dataframes pour ajouter la traduction anglaise et √©viter les jointures\n",
    "data['products'] = data['products'].merge(\n",
    "    data['product_category_name_translation'],\n",
    "    how='left', # Type de jointure : left pour conserver tous les produits m√™me sans traduction\n",
    "    left_on='product_category_name', # Cl√© de jointure dans la table des produits\n",
    "    right_on='product_category_name' # Cl√© de jointure dans la table de traduction\n",
    ")\n",
    "\n",
    "# Renommer la nouvelle colonne pour √©viter les conflits\n",
    "data['products'].rename(columns={'product_category_name_english': 'product_category_name_en'},\n",
    "                inplace=True)\n",
    "\n",
    "# R√©sultat final\n",
    "print(data['products'][['product_category_name', 'product_category_name_en']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59c588",
   "metadata": {},
   "source": [
    "#### 4. Chargement en base de donn√©es\n",
    "\n",
    "\n",
    "Pour le chargement j'ai choisi d'utiliser sqlite3 pour g√©rer la partie cr√©ation de tables en manuel avec des scripts SQL; √ßa permet d'avoir une cr√©ation de tables qui inclue directement toutes les contraintes n√©cessaires, et c'est plus simple que d'utiliser la syntaxe de sqlalchemy, qui ajoute une couche d'abstraction suppl√©mentaire √† SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2bdde31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "DB_PATH = \"./olist.db\"\n",
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Ouverture et ex√©cution du script SQL de cr√©ation de sch√©ma de base de donn√©es\n",
    "    with open('./scripts/schema.sql', 'r') as f:\n",
    "        schema_sql = f.read()\n",
    "        cur.executescript(schema_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfde9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donn√©es nettoy√©es/transform√©es dans la base de donn√©es SQLite\n",
    "for table_name, df in data.items():\n",
    "    df.to_sql(table_name, con=conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f39df0",
   "metadata": {},
   "source": [
    "### 5. D√©veloppement d'indicateurs de perfomances\n",
    "\n",
    "#### üí∞ Ventes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1546356e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 633 rows in 0.905s\n"
     ]
    }
   ],
   "source": [
    "from scripts.utils import benchmark_query\n",
    "\n",
    "with open('./scripts/pki/sales/daily.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "    print(benchmark_query(sql_script))\n",
    "\n",
    "# Performance non optimis√©e: Fetched 633 rows in 0.905s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f57de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 25 rows in 0.716s\n"
     ]
    }
   ],
   "source": [
    "from scripts.utils import benchmark_query\n",
    "\n",
    "with open('./scripts/pki/sales/monthly.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "    benchmark_query(sql_script)\n",
    "    print(benchmark_query(sql_script))\n",
    "\n",
    "# Performance non optimis√©e: Fetched 25 rows in 0.716s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9e07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 3 rows in 0.791s\n"
     ]
    }
   ],
   "source": [
    "from scripts.utils import benchmark_query\n",
    "\n",
    "with open('./scripts/pki/sales/yearly.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "    print(benchmark_query(sql_script))\n",
    "\n",
    "# Performance non optimis√©e: Fetched 3 rows in 0.791s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3b1ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 633 rows in 1.742s\n"
     ]
    }
   ],
   "source": [
    "from scripts.utils import benchmark_query\n",
    "\n",
    "with open('./scripts/pki/sales/previous_year_comparison.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "    print(benchmark_query(sql_script))\n",
    "\n",
    "# Performance non optimis√©e: Fetched 633 rows in 1.742s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ea1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 10 rows in 1.228s\n"
     ]
    }
   ],
   "source": [
    "from scripts.utils import benchmark_query\n",
    "\n",
    "with open('./scripts/pki/sales/top10.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "    print(benchmark_query(sql_script))\n",
    "\n",
    "# Performance non optimis√©e: Fetched 10 rows in 1.228s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9353d7",
   "metadata": {},
   "source": [
    "#### üë• Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5a686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1 rows in 0.141s\n"
     ]
    }
   ],
   "source": [
    "from scripts.utils import benchmark_query\n",
    "\n",
    "with open('./scripts/pki/customers/new_vs_recurring_customers.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "    print(benchmark_query(sql_script))\n",
    "\n",
    "# Performance non optimis√©e: Fetched 1 rows in 0.141s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ea7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1 rows in 0.135s\n"
     ]
    }
   ],
   "source": [
    "from scripts.utils import benchmark_query\n",
    "\n",
    "with open('./scripts/pki/customers/average_cart.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "    print(benchmark_query(sql_script))\n",
    "\n",
    "# Performance non optimis√©e: Fetched 1 rows in 0.135s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1152db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1 rows in 0.027s\n"
     ]
    }
   ],
   "source": [
    "from scripts.utils import benchmark_query\n",
    "\n",
    "with open('./scripts/pki/customers/conversion_rate.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "    print(benchmark_query(sql_script))\n",
    "\n",
    "# Performance non optimis√©e: Fetched 1 rows in 0.027s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e664dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 98666 rows in 1.195s\n"
     ]
    }
   ],
   "source": [
    "from scripts.utils import benchmark_query\n",
    "\n",
    "with open('./scripts/pki/customers/rfmbbq_analysis.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "    print(benchmark_query(sql_script))\n",
    "\n",
    "# Performance non optimis√©e: Fetched 98666 rows in 1.195s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
