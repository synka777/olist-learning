{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f81c0f",
   "metadata": {},
   "source": [
    "## Processing ETL des sets de données Olist\n",
    "\n",
    "#### 1. Nettoyage: Opérations globales à tous les fichiers source\n",
    "\n",
    "Dans cette partie je gère les nettoyages qui peuvent potentiellement s'appliquer à toutes les tables, sans faire de traitement spécifique pour telle ou telle table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08bc36cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathieu/prj/olist-learning/utils.py:11: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col],\n",
      "/home/mathieu/prj/olist-learning/utils.py:11: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col],\n",
      "/home/mathieu/prj/olist-learning/utils.py:11: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col],\n",
      "/home/mathieu/prj/olist-learning/utils.py:11: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col],\n",
      "/home/mathieu/prj/olist-learning/utils.py:11: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col],\n",
      "/home/mathieu/prj/olist-learning/utils.py:11: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col],\n",
      "/home/mathieu/prj/olist-learning/utils.py:11: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[col] = pd.to_datetime(df[col],\n"
     ]
    }
   ],
   "source": [
    "# Import de dépendances et chargement des datasets en mémoire vive\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils import clean_data\n",
    "\n",
    "data = {}\n",
    "\n",
    "rt = './data'\n",
    "csv_file_paths = os.listdir(rt)\n",
    "for csv_path in csv_file_paths:\n",
    "    df = pd.read_csv(f'{rt}/{csv_path}')\n",
    "    df = clean_data(df) # Utilisation d'un heler générique de nettoyage\n",
    "    source = csv_path.replace('olist_', '').replace('.csv', '').replace('_dataset', '')\n",
    "    data[source] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70be187",
   "metadata": {},
   "source": [
    "#### 2. Nettoyage: Cas spécifiques\n",
    "\n",
    "Dans cette partie du notebook j'applique des netoyages plus spécifiques, sur des tables en particulier:\n",
    "- customers: suppression de doublons et d\"une colonne non utilisée\n",
    "- geolocation et customers: normalisation des noms de ville pour cohérence des éventuels PKI qui utiliseraient les noms de ville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d936e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des doublons de données table customers\n",
    "data['customers'] = data['customers'].drop_duplicates(subset=['customer_unique_id'], keep='first')\n",
    "data['customers'] = data['customers'].drop(columns=['customer_unique_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1e48d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import norm\n",
    "\n",
    "# Normalise les noms de villes\n",
    "data['geolocation'] = data['geolocation']['geolocation_city'].apply(norm)\n",
    "data['customers'] = data['customers']['customer_city'].apply(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8045d4",
   "metadata": {},
   "source": [
    "#### 3. Transformations\n",
    "\n",
    "La transformation principale que j'ai imaginée sur ce cas d'école a été d'ajouter les traductions de noms de catégories directement dans la table produits.\n",
    "\n",
    "Le but étant de simplifier les requêtes devant filtrer ou grouper par catégorie, cela évite d'avoir à faire des jointures ultérieures pour récupérer les traductions.\n",
    "\n",
    "Au départ j'ai même considéré faire cela pour cmplètement supprimer la table de traductions, mais j'ai décidé de la garder, dans le scénario où un utilisateur brésilien aurait besoin de faire des requêtes sur la base de données: dans ce cas les jointures resteront faisables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ce17c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_category_name product_category_name_en\n",
      "0             perfumaria                perfumery\n",
      "1                  artes                      art\n",
      "2          esporte_lazer           sports_leisure\n",
      "3                  bebes                     baby\n",
      "4  utilidades_domesticas               housewares\n"
     ]
    }
   ],
   "source": [
    "# Transformation de la table des produits\n",
    "\n",
    "# Merge des dataframes pour ajouter la traduction anglaise et éviter les jointures\n",
    "data['products'] = data['products'].merge(\n",
    "    data['product_category_name_translation'],\n",
    "    how='left', # Type de jointure : left pour conserver tous les produits même sans traduction\n",
    "    left_on='product_category_name', # Clé de jointure dans la table des produits\n",
    "    right_on='product_category_name' # Clé de jointure dans la table de traduction\n",
    ")\n",
    "\n",
    "# Renommer la nouvelle colonne pour éviter les conflits\n",
    "data['products'].rename(columns={'product_category_name_english': 'product_category_name_en'},\n",
    "                inplace=True)\n",
    "\n",
    "# Résultat final\n",
    "print(data['products'][['product_category_name', 'product_category_name_en']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59c588",
   "metadata": {},
   "source": [
    "#### 4. Chargement en base de données\n",
    "\n",
    "\n",
    "Pour le chargement j'ai choisi d'utiliser sqlite3 pour gérer la partie création de tables en manuel avec des scripts SQL; ça permet d'avoir une création de tables qui inclue directement toutes les contraintes nécessaires, et c'est plus simple que d'utiliser la syntaxe de sqlalchemy, qui ajoute une couche d'abstraction supplémentaire à SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2bdde31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "DB_PATH = \"./olist.db\"\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Ouverture et exécution du script SQL de création de schéma de base de données\n",
    "with open('./schema.sql', 'r') as f:\n",
    "    schema_sql = f.read()\n",
    "    cur.executescript(schema_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données nettoyées/transformées dans la base de données SQLite\n",
    "for table_name, df in data.items():\n",
    "    df.to_sql(table_name, con=conn, if_exists='replace', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
